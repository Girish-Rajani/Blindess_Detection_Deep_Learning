{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, optimizers\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data is:  3662\n",
      "Length of testing data is:  1928\n"
     ]
    }
   ],
   "source": [
    "#Load train and test data as dataframes \n",
    "train_dataset = pd.read_csv('./dataset/train.csv')\n",
    "test_dataset = pd.read_csv('./dataset/test.csv')\n",
    "\n",
    "print(\"Length of training data is: \", train_dataset.shape[0])\n",
    "print(\"Length of testing data is: \", test_dataset.shape[0])\n",
    "\n",
    "#debug purpose.\n",
    "#print(\"\\nSummary of first 10 rows of train data:\")\n",
    "#train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "2462\n"
     ]
    }
   ],
   "source": [
    "validation_dataset =train_dataset.iloc[:1200,:]\n",
    "print(validation_dataset.shape[0])\n",
    "\n",
    "#debug purpose.\n",
    "#print(validation_dataset)\n",
    "\n",
    "train_dataset = train_dataset.iloc[1200:,:]\n",
    "print(train_dataset.shape[0])\n",
    "\n",
    "#debug purpose.\n",
    "#print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data in class  0 : 1249\n",
      "Length of training data in class  1 : 244\n",
      "Length of training data in class  2 : 641\n",
      "Length of training data in class  3 : 126\n",
      "Length of training data in class  4 : 202\n",
      "\n",
      "Length of validation data in class  0 : 556\n",
      "Length of validation data in class  1 : 126\n",
      "Length of validation data in class  2 : 358\n",
      "Length of validation data in class  3 : 67\n",
      "Length of validation data in class  4 : 93\n"
     ]
    }
   ],
   "source": [
    "#split the training data into the 5 respective classes using the diagnosis variable from the training data.\n",
    "class_train = []\n",
    "class_val = []\n",
    "\n",
    "for i in range(5):\n",
    "    class_train.append(train_dataset[train_dataset['diagnosis'] == i]['id_code'].tolist())\n",
    "    class_val.append(validation_dataset[validation_dataset['diagnosis'] == i]['id_code'].tolist())\n",
    "\n",
    "#concatenate .png extension at the end of each id to use this as a filename.\n",
    "class_train = [[file_id + '.png' for file_id in class_n] for class_n in class_train]\n",
    "class_val = [[file_id + '.png' for file_id in class_n] for class_n in class_val]\n",
    "\n",
    "for i,class_n in enumerate(class_train):\n",
    "    print(\"Length of training data in class \",i,\":\", len(class_n))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i,class_n in enumerate(class_val):\n",
    "    print(\"Length of validation data in class \",i,\":\", len(class_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original path to images directory\n",
    "train_dataset_dir = './dataset/train_images/'\n",
    "test_dataset_dir = './dataset/test_images/'\n",
    "\n",
    "#new directory\n",
    "new_dataset_dir = './new_dataset/'\n",
    "\n",
    "train_dir = os.path.join(new_dataset_dir, 'train')\n",
    "#create new directories for the labelled training data we have.\n",
    "for i in range(5):\n",
    "    new_folder = os.path.join(train_dir, 'class' + str(i))\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "#create a copy of our training images to the new directories.\n",
    "for i in range(5):\n",
    "    for file_id in class_train[i]:\n",
    "        src = os.path.join(train_dataset_dir, file_id)\n",
    "        dst = os.path.join(train_dir, 'class' +  str(i), file_id)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "val_dir= os.path.join(new_dataset_dir, 'val')\n",
    "#create new directories for the labelled validation data we have.\n",
    "for i in range(5):\n",
    "    new_folder = os.path.join(val_dir, 'class' + str(i))\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "#create a copy of our validation images to the new directories.\n",
    "for i in range(5):\n",
    "    for file_id in class_val[i]:\n",
    "        src = os.path.join(train_dataset_dir, file_id)\n",
    "        dst = os.path.join(val_dir, 'class' +  str(i), file_id)\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 15, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 58,245\n",
      "Trainable params: 57,925\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Build model with convolution layers, pooling, and normalization\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
